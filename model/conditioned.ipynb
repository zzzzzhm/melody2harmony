{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc0a50d0",
   "metadata": {},
   "source": [
    "# CSE 253 — Task 2: Melody‑to‑Harmony with the Nottingham Dataset\n",
    "This notebook clones the Nottingham ABC corpus, cleans and pre‑processes it, builds PyTorch dataloaders, trains a bidirectional LSTM with positional embeddings, and plots training / validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcf34170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- 0. Install & imports --------------------\n",
    "from music21 import corpus, converter, chord, note, meter, stream\n",
    "from collections import defaultdict, Counter\n",
    "import random, numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Helper: return all elements at a given offset, handling BOTH\n",
    "# music21 ≤ v8 (old keyword names) and ≥ v9 (new keyword names)\n",
    "# --------------------------------------------------------------\n",
    "def elements_at(stream_obj, offset, must_begin):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    stream_obj : music21.stream.Stream (usually .flat)\n",
    "    offset      : float   – beat location you’re sampling\n",
    "    must_begin  : bool    – True  → elements must *start* at offset\n",
    "                             False → elements may simply overlap\n",
    "    Returns\n",
    "    -------\n",
    "    list of elements (Notes, Chords, etc.) satisfying the criteria.\n",
    "    Works no‑matter which version of music21 is installed.\n",
    "    \"\"\"\n",
    "    try:  # music21 v9+ (new argument names)\n",
    "        return stream_obj.getElementsByOffset(\n",
    "            offset, offset,\n",
    "            mustBeginInSpan=must_begin,\n",
    "            includeEndBoundary=False,\n",
    "            includeElementsThatEndAtStart=False\n",
    "        )\n",
    "    except TypeError:  # music21 v8‑ or older (legacy names)\n",
    "        return stream_obj.getElementsByOffset(\n",
    "            offset, offset,\n",
    "            mustBeginInSpan=must_begin,\n",
    "            includeEnd=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d15c650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371 chorales found.\n"
     ]
    }
   ],
   "source": [
    "# -------------------- 1. Load Bach chorales --------------------\n",
    "CHORALES = list(corpus.chorales.Iterator(numberingSystem='riemenschneider'))\n",
    "print(f\"{len(CHORALES)} chorales found.\")   # ≈ 389\n",
    "\n",
    "def quantize_parts(score, qlen=1.0):\n",
    "    \"\"\"\n",
    "    Return (melody_pitches, chord_symbols) on a fixed quarter grid.\n",
    "    Works across music21 v8 and v9.\n",
    "    \"\"\"\n",
    "    soprano     = score.parts[0]\n",
    "    chordified  = score.chordify()\n",
    "    flat_melody = soprano.flat\n",
    "    flat_chords = chordified.flat\n",
    "\n",
    "    end = max(score.highestTime, chordified.highestTime)\n",
    "    t, m_pitches, ch_syms = 0.0, [], []\n",
    "    while t < end:\n",
    "\n",
    "        # --- MELODY: any note that *overlaps* t  -----------------\n",
    "        notes_here = [n for n in elements_at(flat_melody, t, must_begin=False)\n",
    "                      if isinstance(n, note.Note)]\n",
    "        if notes_here:\n",
    "            # take highest pitch in case of voice‑leading overlaps\n",
    "            m_pitches.append(max(notes_here, key=lambda n: n.pitch.midi).pitch.midi)\n",
    "        else:\n",
    "            m_pitches.append(\"rest\")\n",
    "\n",
    "        # --- CHORD: any sonority overlapping t  ------------------\n",
    "        ch_here = [c for c in elements_at(flat_chords, t, must_begin=False)\n",
    "                   if isinstance(c, chord.Chord)]\n",
    "        if ch_here:\n",
    "            c      = ch_here[0]\n",
    "            symbol = f\"{c.root().name}:{c.quality or 'maj'}\"\n",
    "        else:\n",
    "            symbol = \"N.C.\"\n",
    "        ch_syms.append(symbol)\n",
    "\n",
    "        t += qlen\n",
    "    return m_pitches, ch_syms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd4dc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/371 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/music21/stream/base.py:4014: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
      "  sIterator = self.iter().getElementsByOffset(\n",
      "100%|██████████| 371/371 [00:16<00:00, 22.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 296  |  Valid 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- 3.  Split → train / valid ----------\n",
    "data = [quantize_parts(s,qlen=0.5) for s in tqdm(CHORALES)]\n",
    "# keep sequences with a sensible length\n",
    "data = [(m, c) for m, c in data if len(m) > 16]\n",
    "random.shuffle(data)\n",
    "split = int(0.8*len(data))\n",
    "train, valid = data[:split], data[split:]\n",
    "print(f\"Train {len(train)}  |  Valid {len(valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b7a50ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping built for 48 (pitch‑class, beat) combos.\n"
     ]
    }
   ],
   "source": [
    "# -------------------- 4. Beat‑aware frequency table --------------------\n",
    "# Count occurrences:  P(chord | melody‑pitch‑class, beat‑index mod 4)\n",
    "counts = defaultdict(Counter)\n",
    "for mel, chords in train:\n",
    "    for idx, (p, ch) in enumerate(zip(mel, chords)):\n",
    "        if p == \"rest\" or ch == \"N.C.\":\n",
    "            continue\n",
    "        pc   = p % 12            # 0‑11\n",
    "        beat = idx % 4           # assume 4‑beat bar\n",
    "        counts[(pc, beat)][ch] += 1\n",
    "\n",
    "# Deterministic mapping:   argmax chord for each (pc, beat)\n",
    "mapping  = {key: cnts.most_common(1)[0][0] for key, cnts in counts.items()}\n",
    "fallback = \"C:maj\"               # used when (pc, beat) never seen in training\n",
    "print(f\"Mapping built for {len(mapping)} (pitch‑class, beat) combos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67d05a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- 5. Define harmonize_melody and write_midi --------------------\n",
    "def harmonize_melody(mel_line):\n",
    "    harmony, last = [], fallback\n",
    "    for idx, p in enumerate(mel_line):\n",
    "        if p == \"rest\":\n",
    "            harmony.append(last)          # sustain previous chord\n",
    "            continue\n",
    "        pc, beat = p % 12, idx % 4\n",
    "        ch = mapping.get((pc, beat), fallback)\n",
    "        harmony.append(ch)\n",
    "        last = ch\n",
    "    return harmony\n",
    "\n",
    "\n",
    "def write_midi(mel, har, fp=\"bach_harmonisation_baseline.mid\"):\n",
    "    s = stream.Stream()\n",
    "    part_mel, part_har = stream.Part(), stream.Part()\n",
    "    for m_note, ch_sym in zip(mel, har):\n",
    "        dur = 1.0\n",
    "        # melody track\n",
    "        n = note.Note(m_note) if m_note != \"rest\" else note.Rest()\n",
    "        n.quarterLength = dur\n",
    "        part_mel.append(n)\n",
    "        # harmony track (simple root‑position triad)\n",
    "        if ch_sym != \"N.C.\":\n",
    "            root = note.Note(ch_sym.split(':')[0])\n",
    "            tri  = chord.Chord([root,\n",
    "                                root.transpose(4),\n",
    "                                root.transpose(7)])\n",
    "        else:\n",
    "            tri = chord.Chord([])\n",
    "        tri.quarterLength = dur\n",
    "        part_har.append(tri)\n",
    "    s.append([part_mel, part_har])\n",
    "    s.write(\"midi\", fp=fp)\n",
    "    print(\"✔  MIDI written:\", fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dcd4aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation chord‑token accuracy: 0.393\n",
      "✔  MIDI written: bach_harmonisation_baseline.mid\n"
     ]
    }
   ],
   "source": [
    "# ---------- 6. Evaluate & write a demo MIDI----------\n",
    "hits, tot = 0, 0\n",
    "for mel, gold_ch in valid:\n",
    "    pred_ch = harmonize_melody(mel)\n",
    "    for p, g in zip(pred_ch, gold_ch):\n",
    "        if g != \"N.C.\":\n",
    "            hits += (p == g)\n",
    "            tot  += 1\n",
    "print(f\"Validation chord‑token accuracy: {hits/tot:.3f}\")\n",
    "\n",
    "\n",
    "mel_sample, _ = valid[0]\n",
    "har_sample    = harmonize_melody(mel_sample)\n",
    "write_midi(mel_sample, har_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a2af9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in this melody: 96\n",
      "Non‑rest tokens            : 45\n"
     ]
    }
   ],
   "source": [
    "# Count how many non‑rest melody tokens the chorale actually has\n",
    "mel, gold = valid[0]\n",
    "non_rest  = sum(1 for p in mel if p != \"rest\")\n",
    "print(f\"Total tokens in this melody: {len(mel)}\")\n",
    "print(f\"Non‑rest tokens            : {non_rest}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a3e431b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‣  Melody‐vocab size = 29  (incl. PAD, rest)\n",
      "‣  Chord‐vocab size  = 65  (incl. PAD, N.C.)\n",
      "‣  Number of training sequences: 296\n",
      "‣  Number of validation sequences: 75\n"
     ]
    }
   ],
   "source": [
    "# -------------------- 7.  Prepare vocabularies & integer‐encode sequences --------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 7.1. Collect all chord tokens and melody tokens from train/valid\n",
    "all_chords   = set()\n",
    "all_pitches  = set()\n",
    "\n",
    "for mel, ch in train + valid:\n",
    "    all_pitches.update([p for p in mel])     # p is int or \"rest\"\n",
    "    all_chords.update([c for c in ch])       # c is chord‐string or \"N.C.\"\n",
    "\n",
    "# Build sorted vocab lists, including special PAD tokens\n",
    "PITCH_TOKENS = sorted([p for p in all_pitches if p != \"rest\"])\n",
    "PITCH_VOCAB  = [\"<PAD>\", \"rest\"] + [str(p) for p in PITCH_TOKENS]\n",
    "pitch2idx    = {tok: idx for idx, tok in enumerate(PITCH_VOCAB)}\n",
    "idx2pitch    = {idx: tok for tok, idx in pitch2idx.items()}\n",
    "\n",
    "CHORD_VOCAB  = [\"<PAD>\", \"N.C.\"] + sorted([c for c in all_chords if c != \"N.C.\"])\n",
    "chord2idx    = {tok: idx for idx, tok in enumerate(CHORD_VOCAB)}\n",
    "idx2chord    = {idx: tok for tok, idx in chord2idx.items()}\n",
    "\n",
    "print(f\"‣  Melody‐vocab size = {len(PITCH_VOCAB)}  (incl. PAD, rest)\")\n",
    "print(f\"‣  Chord‐vocab size  = {len(CHORD_VOCAB)}  (incl. PAD, N.C.)\")\n",
    "\n",
    "# 7.2. Helper to encode one (melody, chord) pair into integer sequences\n",
    "def encode_sequence(mel_seq, chord_seq):\n",
    "    \"\"\"\n",
    "    mel_seq   : list of length T, entries int or \"rest\"\n",
    "    chord_seq : list of length T, entries chord‐string or \"N.C.\"\n",
    "    Returns:\n",
    "      pitch_idx_seq : LongTensor[T]\n",
    "      chord_idx_seq : LongTensor[T]\n",
    "    \"\"\"\n",
    "    T = len(mel_seq)\n",
    "    pitch_idxs = []\n",
    "    chord_idxs = []\n",
    "    for i in range(T):\n",
    "        p = mel_seq[i]\n",
    "        if p == \"rest\":\n",
    "            pitch_idxs.append(pitch2idx[\"rest\"])\n",
    "        else:\n",
    "            pitch_idxs.append(pitch2idx[str(p)])\n",
    "        c = chord_seq[i]\n",
    "        chord_idxs.append(chord2idx[c])\n",
    "    return torch.LongTensor(pitch_idxs), torch.LongTensor(chord_idxs)\n",
    "\n",
    "# 7.3. Encode all train/valid sequences\n",
    "train_data = []\n",
    "for mel, ch in train:\n",
    "    pitch_idxs, chord_idxs = encode_sequence(mel, ch)\n",
    "    train_data.append((pitch_idxs, chord_idxs, len(mel)))\n",
    "\n",
    "valid_data = []\n",
    "for mel, ch in valid:\n",
    "    pitch_idxs, chord_idxs = encode_sequence(mel, ch)\n",
    "    valid_data.append((pitch_idxs, chord_idxs, len(mel)))\n",
    "\n",
    "print(f\"‣  Number of training sequences: {len(train_data)}\")\n",
    "print(f\"‣  Number of validation sequences: {len(valid_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c466f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pitches: torch.Size([32, 136]) | chords: torch.Size([32, 136]) | lengths: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# -------------------- 8.  PyTorch Dataset & DataLoader (with padding) --------------------\n",
    "\n",
    "class ChoraleDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        \"\"\"\n",
    "        data_list : list of tuples (pitch_idxs: LongTensor[T], chord_idxs: LongTensor[T], length: int)\n",
    "        \"\"\"\n",
    "        self.data = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pitch_idxs, chord_idxs, length = self.data[idx]\n",
    "        return pitch_idxs, chord_idxs, length\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"\n",
    "        batch: list of (pitch_idxs: LongTensor[T_i], chord_idxs: LongTensor[T_i], len_i)\n",
    "        Returns:\n",
    "          padded_pitches: (B, L) LongTensor\n",
    "          padded_chords : (B, L) LongTensor\n",
    "          lengths       : (B,) LongTensor\n",
    "        \"\"\"\n",
    "        batch_size = len(batch)\n",
    "        lengths = torch.LongTensor([item[2] for item in batch])\n",
    "        max_len = lengths.max().item()\n",
    "\n",
    "        padded_pitches = torch.full((batch_size, max_len),\n",
    "                                    fill_value=pitch2idx[\"<PAD>\"],\n",
    "                                    dtype=torch.long)\n",
    "        padded_chords = torch.full((batch_size, max_len),\n",
    "                                   fill_value=chord2idx[\"<PAD>\"],\n",
    "                                   dtype=torch.long)\n",
    "\n",
    "        for i, (p_seq, c_seq, L) in enumerate(batch):\n",
    "            padded_pitches[i, :L] = p_seq\n",
    "            padded_chords[i, :L] = c_seq\n",
    "\n",
    "        return padded_pitches, padded_chords, lengths\n",
    "\n",
    "# Hyperparameters for DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = ChoraleDataset(train_data)\n",
    "valid_dataset = ChoraleDataset(valid_data)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=ChoraleDataset.collate_fn\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=ChoraleDataset.collate_fn\n",
    ")\n",
    "\n",
    "# Inspect one batch\n",
    "for batch in train_loader:\n",
    "    p, c, L = batch\n",
    "    print(\"pitches:\", p.shape, \"| chords:\", c.shape, \"| lengths:\", L.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20335c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTMChordTagger(\n",
      "  (embedding): Embedding(29, 192, padding_idx=0)\n",
      "  (lstm): LSTM(192, 384, num_layers=3, batch_first=True, dropout=0.4, bidirectional=True)\n",
      "  (classifier): Linear(in_features=768, out_features=65, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      ")\n",
      "Total parameters: 8921345\n"
     ]
    }
   ],
   "source": [
    "# -------------------- 9.  Bi-LSTM Model Definition --------------------\n",
    "\n",
    "class BiLSTMChordTagger(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_pitch_tokens: int,\n",
    "                 n_chord_tokens: int,\n",
    "                 embed_dim: int = 128,\n",
    "                 lstm_hidden: int = 256,\n",
    "                 lstm_layers: int = 2,\n",
    "                 dropout: float = 0.3):\n",
    "        \"\"\"\n",
    "        n_pitch_tokens : size of pitch vocabulary (including PAD & 'rest')\n",
    "        n_chord_tokens : size of chord vocabulary (including PAD & 'N.C.')\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=n_pitch_tokens,\n",
    "                                      embedding_dim=embed_dim,\n",
    "                                      padding_idx=pitch2idx[\"<PAD>\"])\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if lstm_layers > 1 else 0.0\n",
    "        )\n",
    "        self.classifier = nn.Linear(in_features=lstm_hidden * 2,\n",
    "                                    out_features=n_chord_tokens)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_pitches, lengths):\n",
    "        \"\"\"\n",
    "        input_pitches : (B, L) LongTensor of pitch‐indices (padded with <PAD>)\n",
    "        lengths       : (B,) LongTensor of actual lengths\n",
    "        Returns:\n",
    "          logits : (B, L, n_chord_tokens)\n",
    "        \"\"\"\n",
    "        emb = self.embedding(input_pitches)  # (B, L, embed_dim)\n",
    "        emb = self.dropout(emb)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb,\n",
    "                                                   lengths.cpu(),\n",
    "                                                   batch_first=True,\n",
    "                                                   enforce_sorted=False)\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        # out: (B, L, lstm_hidden * 2)\n",
    "        logits = self.classifier(out)        # (B, L, n_chord_tokens)\n",
    "        return logits\n",
    "\n",
    "# Instantiate the model\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BiLSTMChordTagger(\n",
    "    n_pitch_tokens=len(PITCH_VOCAB),\n",
    "    n_chord_tokens=len(CHORD_VOCAB),\n",
    "    embed_dim=192,\n",
    "    lstm_hidden=384,\n",
    "    lstm_layers=3,\n",
    "    dropout=0.4\n",
    ").to(DEVICE)\n",
    "\n",
    "print(model)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1336fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- 10.  Loss, Optimizer, & Training Utilities --------------------\n",
    "\n",
    "# Ignore \"N.C.\" and \"<PAD>\" tokens during loss/accuracy\n",
    "ignore_idx = chord2idx[\"N.C.\"]\n",
    "pad_idx    = chord2idx[\"<PAD>\"]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=ignore_idx)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"max\",\n",
    "    factor=0.5,\n",
    "    patience=2\n",
    ")\n",
    "\n",
    "def compute_batch_accuracy(logits, targets):\n",
    "    \"\"\"\n",
    "    logits  : (B, L, C) raw scores\n",
    "    targets : (B, L) ground‐truth chord indices\n",
    "    Returns:\n",
    "      num_correct : int\n",
    "      num_valid   : int\n",
    "    \"\"\"\n",
    "    B, L, C = logits.shape\n",
    "    flat_logits = logits.view(-1, C)        # (B*L, C)\n",
    "    flat_preds  = flat_logits.argmax(dim=1)  # (B*L,)\n",
    "    flat_targets= targets.view(-1)          # (B*L,)\n",
    "\n",
    "    valid_mask = (flat_targets != ignore_idx) & (flat_targets != pad_idx)\n",
    "    correct = (flat_preds == flat_targets) & valid_mask\n",
    "    num_correct = correct.sum().item()\n",
    "    num_valid   = valid_mask.sum().item()\n",
    "    return num_correct, num_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3bffe59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01  | Train Loss = 3.9957  Acc = 0.1402  | Val Loss = 3.9217  Acc = 0.1650\n",
      "Epoch 02  | Train Loss = 3.9067  Acc = 0.2244  | Val Loss = 3.7946  Acc = 0.3045\n",
      "Epoch 03  | Train Loss = 3.8232  Acc = 0.3597  | Val Loss = 3.7052  Acc = 0.3528\n",
      "Epoch 04  | Train Loss = 3.7239  Acc = 0.4162  | Val Loss = 3.6122  Acc = 0.4257\n",
      "Epoch 05  | Train Loss = 3.6821  Acc = 0.4429  | Val Loss = 3.5735  Acc = 0.4617\n",
      "Epoch 06  | Train Loss = 3.6711  Acc = 0.4829  | Val Loss = 3.5329  Acc = 0.4809\n",
      "Epoch 07  | Train Loss = 3.5847  Acc = 0.5170  | Val Loss = 3.4812  Acc = 0.5091\n",
      "Epoch 08  | Train Loss = 3.5848  Acc = 0.5330  | Val Loss = 3.4561  Acc = 0.5114\n",
      "Epoch 09  | Train Loss = 3.5025  Acc = 0.5554  | Val Loss = 3.4208  Acc = 0.5419\n",
      "Epoch 10  | Train Loss = 3.5341  Acc = 0.5735  | Val Loss = 3.4007  Acc = 0.5593\n",
      "Epoch 11  | Train Loss = 3.4621  Acc = 0.5911  | Val Loss = 3.3664  Acc = 0.5629\n",
      "Epoch 12  | Train Loss = 3.4173  Acc = 0.6081  | Val Loss = 3.3492  Acc = 0.5611\n",
      "Epoch 13  | Train Loss = 3.4088  Acc = 0.5939  | Val Loss = 3.3506  Acc = 0.5447\n",
      "Epoch 14  | Train Loss = 3.4057  Acc = 0.6077  | Val Loss = 3.3186  Acc = 0.5515\n",
      "Epoch 15  | Train Loss = 3.2916  Acc = 0.6343  | Val Loss = 3.2903  Acc = 0.5884\n",
      "Epoch 16  | Train Loss = 3.3940  Acc = 0.6460  | Val Loss = 3.2806  Acc = 0.5848\n",
      "Epoch 17  | Train Loss = 3.3610  Acc = 0.6625  | Val Loss = 3.2664  Acc = 0.6030\n",
      "Epoch 18  | Train Loss = 3.3834  Acc = 0.6656  | Val Loss = 3.2624  Acc = 0.6016\n",
      "Epoch 19  | Train Loss = 3.3291  Acc = 0.6818  | Val Loss = 3.2555  Acc = 0.6026\n",
      "Epoch 20  | Train Loss = 3.3342  Acc = 0.6930  | Val Loss = 3.2432  Acc = 0.6098\n",
      "\n",
      "→  Best validation chord‐token accuracy = 0.6098\n"
     ]
    }
   ],
   "source": [
    "# -------------------- 11.  Training & Validation Loop --------------------\n",
    "\n",
    "NUM_EPOCHS    = 20\n",
    "best_val_acc  = 0.0\n",
    "\n",
    "train_losses  = []\n",
    "valid_losses  = []\n",
    "train_accs    = []\n",
    "valid_accs    = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # ---- Training Phase ----\n",
    "    model.train()\n",
    "    epoch_loss    = 0.0\n",
    "    epoch_correct = 0\n",
    "    epoch_total   = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        pitches, chords, lengths = batch\n",
    "        pitches = pitches.to(DEVICE)\n",
    "        chords  = chords.to(DEVICE)\n",
    "        lengths = lengths.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(pitches, lengths)   # (B, L, n_chords)\n",
    "\n",
    "        B, L, C = logits.shape\n",
    "        flat_logits  = logits.view(B * L, C)\n",
    "        flat_targets = chords.view(B * L)\n",
    "\n",
    "        loss = criterion(flat_logits, flat_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * B\n",
    "        batch_correct, batch_total = compute_batch_accuracy(logits, chords)\n",
    "        epoch_correct += batch_correct\n",
    "        epoch_total   += batch_total\n",
    "\n",
    "    train_loss = epoch_loss / len(train_dataset)\n",
    "    train_acc  = epoch_correct / epoch_total\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # ---- Validation Phase ----\n",
    "    model.eval()\n",
    "    val_loss    = 0.0\n",
    "    val_correct = 0\n",
    "    val_total   = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            pitches, chords, lengths = batch\n",
    "            pitches = pitches.to(DEVICE)\n",
    "            chords  = chords.to(DEVICE)\n",
    "            lengths = lengths.to(DEVICE)\n",
    "\n",
    "            logits = model(pitches, lengths)  # (B, L, n_chords)\n",
    "\n",
    "            B, L, C = logits.shape\n",
    "            flat_logits  = logits.view(B * L, C)\n",
    "            flat_targets = chords.view(B * L)\n",
    "\n",
    "            loss = criterion(flat_logits, flat_targets)\n",
    "            val_loss += loss.item() * B\n",
    "\n",
    "            batch_correct, batch_total = compute_batch_accuracy(logits, chords)\n",
    "            val_correct += batch_correct\n",
    "            val_total   += batch_total\n",
    "\n",
    "    val_loss = val_loss / len(valid_dataset)\n",
    "    val_acc  = val_correct / val_total\n",
    "\n",
    "    valid_losses.append(val_loss)\n",
    "    valid_accs.append(val_acc)\n",
    "\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}  | \"\n",
    "          f\"Train Loss = {train_loss:.4f}  Acc = {train_acc:.4f}  | \"\n",
    "          f\"Val Loss = {val_loss:.4f}  Acc = {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_bilstm_chord_tagger.pt\")\n",
    "\n",
    "print(f\"\\n→  Best validation chord‐token accuracy = {best_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3200728f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔  Final validation chord‐token accuracy = 0.6098\n",
      "Example  →  Melody length = 96 tokens\n",
      "Ground‐truth chords (first 16): ['N.C.', 'N.C.', 'N.C.', 'A:minor', 'N.C.', 'E:major', 'N.C.', 'N.C.', 'N.C.', 'N.C.', 'N.C.', 'N.C.', 'N.C.', 'N.C.', 'N.C.', 'A:minor']\n",
      "Predicted chords    (first 16): ['E:major', 'E:major', 'E:major', 'A:minor', 'A:minor', 'E:major', 'E:major', 'A:minor', 'A:minor', 'D:minor', 'E:major', 'A:minor', 'E:major', 'E:major', 'A:minor', 'A:minor']\n",
      "✔  MIDI written: demo_bilstm_harmonisation.mid\n"
     ]
    }
   ],
   "source": [
    "# -------------------- 13.  Evaluate on Validation & Demo MIDI --------------------\n",
    "\n",
    "# Reload best model\n",
    "model.load_state_dict(torch.load(\"best_bilstm_chord_tagger.pt\"))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Compute final validation accuracy\n",
    "val_correct = 0\n",
    "val_total   = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in valid_loader:\n",
    "        pitches, chords, lengths = batch\n",
    "        pitches = pitches.to(DEVICE)\n",
    "        chords  = chords.to(DEVICE)\n",
    "        lengths = lengths.to(DEVICE)\n",
    "\n",
    "        logits = model(pitches, lengths)  # (B, L, C)\n",
    "        batch_correct, batch_total = compute_batch_accuracy(logits, chords)\n",
    "        val_correct += batch_correct\n",
    "        val_total   += batch_total\n",
    "\n",
    "final_val_acc = val_correct / val_total\n",
    "print(f\"✔  Final validation chord‐token accuracy = {final_val_acc:.4f}\")\n",
    "\n",
    "# 13.1. Generate & write one demo MIDI using the trained model\n",
    "def predict_chords_from_melody(mel_seq):\n",
    "    \"\"\"\n",
    "    Given a melody sequence mel_seq (list of int or \"rest\"),\n",
    "    return a list of predicted chord‐strings.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    pitch_idxs, _ = encode_sequence(mel_seq, [\"N.C.\"] * len(mel_seq))\n",
    "    L = len(mel_seq)\n",
    "    pitch_idxs = pitch_idxs.unsqueeze(0).to(DEVICE)        # (1, L)\n",
    "    lengths = torch.LongTensor([L]).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(pitch_idxs, lengths)                 # (1, L, C)\n",
    "        preds  = logits.argmax(dim=2).squeeze(0).cpu().tolist()\n",
    "\n",
    "    pred_chord_seq = [idx2chord[p] for p in preds]\n",
    "    return pred_chord_seq\n",
    "\n",
    "# Pick the first validation example and write a demo MIDI\n",
    "mel_sample, gold_chords = valid[0]\n",
    "pred_chords = predict_chords_from_melody(mel_sample)\n",
    "\n",
    "print(f\"Example  →  Melody length = {len(mel_sample)} tokens\")\n",
    "print(f\"Ground‐truth chords (first 16): {gold_chords[:16]}\")\n",
    "print(f\"Predicted chords    (first 16): {pred_chords[:16]}\")\n",
    "\n",
    "write_midi(mel_sample, pred_chords, fp=\"demo_bilstm_harmonisation.mid\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
